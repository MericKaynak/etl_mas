{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68725e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "import os\n",
    "import math\n",
    "import streamlit as st\n",
    "\n",
    "class SchemaAnalyzer:\n",
    "    \"\"\"\n",
    "    Universal schema analyzer, der JSON, CSV, Excel und XML erkennt und analysiert.\n",
    "    Unterstützt jetzt die rekursive Analyse von Arrays, um detaillierte Schemas \n",
    "    für verschachtelte Objekte zu erstellen.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, file_path, rows_per_sample_section=20):\n",
    "        if not os.path.exists(file_path):\n",
    "            raise FileNotFoundError(f\"Die Datei wurde nicht gefunden: {file_path}\")\n",
    "        self.file_path = file_path\n",
    "        self.sample_chunk_size = rows_per_sample_section\n",
    "        self.extension = os.path.splitext(file_path)[1].lower()\n",
    "\n",
    "    def analyze(self):\n",
    "        \"\"\"Bestimmt den Datentyp und das Format automatisch und analysiert das Schema.\"\"\"\n",
    "        try:\n",
    "            if self.extension == \".json\":\n",
    "                return self._analyze_json()\n",
    "            elif self.extension == \".csv\":\n",
    "                return self._analyze_csv()\n",
    "            elif self.extension in [\".xls\", \".xlsx\"]:\n",
    "                return self._analyze_excel()\n",
    "            elif self.extension == \".xml\":\n",
    "                return self._analyze_xml()\n",
    "            else:\n",
    "                return {\"error\": f\"Unsupported file type: {self.extension}\"}\n",
    "        except Exception as e:\n",
    "            import traceback\n",
    "            return {\"error\": f\"An error occurred: {e}\", \"traceback\": traceback.format_exc()}\n",
    "\n",
    "    def _determine_type(self, value):\n",
    "        \"\"\"Bestimmt den Datentyp eines Wertes, mit rekursiver Analyse für Listen und Dictionaries.\"\"\"\n",
    "        if isinstance(value, dict):\n",
    "            return self._analyze_schema(value)\n",
    "        elif isinstance(value, list):\n",
    "            if not value:\n",
    "                return [\"array of\", \"empty\"]\n",
    "            # Analysiere den Typ jedes Elements in der Liste\n",
    "            element_types = [self._determine_type(item) for item in value]\n",
    "            # Fasse die gefundenen Typen zusammen (dies ist die neue, leistungsfähige Logik)\n",
    "            merged_types = self._merge_types(element_types)\n",
    "            return [\"array of\"] + merged_types\n",
    "        elif isinstance(value, str):\n",
    "            return \"string\"\n",
    "        elif isinstance(value, int):\n",
    "            return \"integer\"\n",
    "        elif isinstance(value, float):\n",
    "            return \"number\"\n",
    "        elif isinstance(value, bool):\n",
    "            return \"boolean\"\n",
    "        elif value is None:\n",
    "            return \"null\"\n",
    "        else:\n",
    "            return \"unknown\"\n",
    "\n",
    "    def _infer_value_type(self, value: str):\n",
    "        \"\"\"Leitet den Datentyp aus einem String ab (für CSV/Excel).\"\"\"\n",
    "        if not isinstance(value, str) or value.strip() == '':\n",
    "            return self._determine_type(value)\n",
    "        try: int(value); return \"integer\"\n",
    "        except (ValueError, TypeError): pass\n",
    "        try: float(value); return \"number\"\n",
    "        except (ValueError, TypeError): pass\n",
    "        if value.lower() in ['true', 'false']: return \"boolean\"\n",
    "        return \"string\"\n",
    "\n",
    "    def _merge_schemas(self, list_of_schemas):\n",
    "        \"\"\"Führt eine Liste von Schema-Dictionaries zu einem einzigen Schema zusammen.\"\"\"\n",
    "        if not list_of_schemas:\n",
    "            return {}\n",
    "            \n",
    "        # Sammle alle Schlüssel aus allen Schemas\n",
    "        all_keys = set()\n",
    "        for schema in list_of_schemas:\n",
    "            if isinstance(schema, dict):\n",
    "                all_keys.update(schema.keys())\n",
    "\n",
    "        merged_schema = {}\n",
    "        for key in all_keys:\n",
    "            # Sammle alle Typen für diesen Schlüssel aus allen Schemas\n",
    "            types_for_key = []\n",
    "            for schema in list_of_schemas:\n",
    "                if isinstance(schema, dict) and key in schema:\n",
    "                    types_for_key.append(schema[key])\n",
    "            \n",
    "            # Führe die gesammelten Typen rekursiv zusammen\n",
    "            merged_schema[key] = self._merge_types(types_for_key)[0] if len(self._merge_types(types_for_key)) == 1 else self._merge_types(types_for_key)\n",
    "            \n",
    "        return merged_schema\n",
    "\n",
    "    def _merge_types(self, types):\n",
    "        \"\"\"\n",
    "        Führt eine Liste von Typ-Definitionen zusammen. Kann simple Typen (string),\n",
    "        Objekt-Schemas (dict) und gemischte Typen verarbeiten.\n",
    "        \"\"\"\n",
    "        simple_types = set()\n",
    "        schemas = []\n",
    "        \n",
    "        for t in types:\n",
    "            if isinstance(t, dict):\n",
    "                schemas.append(t)\n",
    "            elif isinstance(t, list) and t and t[0] == \"array of\":\n",
    "                # Behandelt Arrays in Arrays\n",
    "                # Vereinfachte Darstellung, um unendliche Komplexität zu vermeiden\n",
    "                simple_types.add(\"array\") \n",
    "            elif isinstance(t, list):\n",
    "                # Flache Listen von Typen\n",
    "                for inner_t in t:\n",
    "                    if isinstance(inner_t, dict):\n",
    "                        schemas.append(inner_t)\n",
    "                    else:\n",
    "                        simple_types.add(inner_t)\n",
    "            elif t is not None:\n",
    "                simple_types.add(t)\n",
    "\n",
    "        final_types = sorted(list(simple_types))\n",
    "        \n",
    "        if schemas:\n",
    "            merged_schema = self._merge_schemas(schemas)\n",
    "            final_types.append(merged_schema)\n",
    "            \n",
    "        if not final_types:\n",
    "            return [\"unknown\"]\n",
    "\n",
    "        return final_types\n",
    "\n",
    "    def _analyze_schema(self, data):\n",
    "        \"\"\"Analysiert das Schema eines einzelnen Dictionaries.\"\"\"\n",
    "        if not isinstance(data, dict):\n",
    "            return self._determine_type(data)\n",
    "        schema = {}\n",
    "        for key, value in data.items():\n",
    "            schema[key] = self._determine_type(value)\n",
    "        return schema\n",
    "    \n",
    "    # --- NEUE METHODE ---\n",
    "    \n",
    "    def get_file_snippets(self, n):\n",
    "        \"\"\"\n",
    "        Liest die ersten n, die mittleren n und die letzten n Zeilen einer Datei\n",
    "        und gibt sie als Strings zurück.\n",
    "\n",
    "        Args:\n",
    "            n (int): Die Anzahl der Zeilen, die aus jedem Abschnitt geholt werden sollen.\n",
    "\n",
    "        Returns:\n",
    "            dict: Ein Dictionary mit den Schlüsseln 'head', 'middle', und 'tail',\n",
    "                  das die entsprechenden Dateiabschnitte als String enthält.\n",
    "                  Bei sehr kleinen Dateien können sich diese Abschnitte überschneiden.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Versuche, die Datei mit UTF-8 zu öffnen, falle auf Latin-1 zurück, falls ein Fehler auftritt\n",
    "            try:\n",
    "                with open(self.file_path, 'r', encoding='utf-8') as f:\n",
    "                    lines = f.readlines()\n",
    "            except UnicodeDecodeError:\n",
    "                with open(self.file_path, 'r', encoding='latin1') as f:\n",
    "                    lines = f.readlines()\n",
    "\n",
    "            total_lines = len(lines)\n",
    "\n",
    "            # Extrahiere den Anfang (head)\n",
    "            head_lines = lines[:n]\n",
    "            head_str = \"\".join(head_lines)\n",
    "\n",
    "            # Extrahiere das Ende (tail)\n",
    "            tail_lines = lines[-n:]\n",
    "            tail_str = \"\".join(tail_lines)\n",
    "\n",
    "            # Extrahiere die Mitte (middle)\n",
    "            if total_lines <= n:\n",
    "                # Wenn die Datei kürzer als n ist, ist der mittlere Teil der gesamte Inhalt\n",
    "                middle_lines = lines\n",
    "            else:\n",
    "                middle_start_index = max(0, (total_lines - n) // 2)\n",
    "                middle_lines = lines[middle_start_index : middle_start_index + n]\n",
    "            middle_str = \"\".join(middle_lines)\n",
    "            \n",
    "            return {\n",
    "                \"head\": head_str,\n",
    "                \"middle\": middle_str,\n",
    "                \"tail\": tail_str\n",
    "            }\n",
    "        except Exception as e:\n",
    "            import traceback\n",
    "            return {\n",
    "                \"error\": f\"Fehler beim Lesen der Datei: {e}\",\n",
    "                \"traceback\": traceback.format_exc()\n",
    "            }\n",
    "\n",
    "    # --- Format-spezifische Parser und Hilfsmethoden ---\n",
    "    \n",
    "    def _get_strategic_samples(self, all_rows):\n",
    "        \"\"\"Nimmt Stichproben vom Anfang, der Mitte und dem Ende.\"\"\"\n",
    "        total_rows = len(all_rows)\n",
    "        chunk_size = self.sample_chunk_size\n",
    "        if total_rows <= chunk_size * 2:\n",
    "            return all_rows\n",
    "        head = all_rows[:chunk_size]\n",
    "        middle_start = max(chunk_size, (total_rows - chunk_size) // 2)\n",
    "        middle = all_rows[middle_start : middle_start + chunk_size]\n",
    "        tail = all_rows[-chunk_size:]\n",
    "        samples = head + middle + tail\n",
    "        seen_representations = set()\n",
    "        unique_samples = []\n",
    "        for row in samples:\n",
    "            try:\n",
    "                representation = json.dumps(row, sort_keys=True)\n",
    "                if representation not in seen_representations:\n",
    "                    seen_representations.add(representation)\n",
    "                    unique_samples.append(row)\n",
    "            except TypeError:\n",
    "                rep_str = str(row)\n",
    "                if rep_str not in seen_representations:\n",
    "                    seen_representations.add(rep_str)\n",
    "                    unique_samples.append(row)\n",
    "        return unique_samples\n",
    "\n",
    "    def _analyze_json(self):\n",
    "        with open(self.file_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        return self._determine_type(data)\n",
    "\n",
    "    def _analyze_combined_samples(self, samples, use_infer_type=False):\n",
    "        \"\"\"Analysiert eine Liste von Zeilen (aus CSV/Excel).\"\"\"\n",
    "        if not samples:\n",
    "            return {\"info\": \"No data samples found to analyze.\"}\n",
    "        \n",
    "        # Dies ist im Grunde eine Analyse eines Arrays von Objekten\n",
    "        return self._determine_type(samples)\n",
    "\n",
    "    def _analyze_csv(self):\n",
    "        try:\n",
    "            df = pd.read_csv(self.file_path, low_memory=False, keep_default_na=False)\n",
    "        except Exception:\n",
    "            df = pd.read_csv(self.file_path, low_memory=False, keep_default_na=False, encoding='latin1')\n",
    "        if df.empty:\n",
    "            return {\"info\": \"CSV is empty or header-only\"}\n",
    "        # Konvertiere alles in native Python-Typen, wo möglich\n",
    "        rows = df.to_dict(orient='records')\n",
    "        typed_rows = []\n",
    "        for row in rows:\n",
    "            typed_row = {k: self._infer_value_type(v) if isinstance(v, str) else v for k, v in row.items()}\n",
    "            typed_rows.append(typed_row)\n",
    "\n",
    "        samples = self._get_strategic_samples(typed_rows)\n",
    "        return self._analyze_combined_samples(samples, use_infer_type=False)\n",
    "\n",
    "    def _analyze_excel(self):\n",
    "        df = pd.read_excel(self.file_path, keep_default_na=False)\n",
    "        if df.empty: return {\"info\": \"Excel sheet is empty\"}\n",
    "        df = df.astype(str)\n",
    "        rows = df.to_dict(orient='records')\n",
    "        typed_rows = []\n",
    "        for row in rows:\n",
    "            typed_row = {k: self._infer_value_type(v) for k,v in row.items()}\n",
    "            typed_rows.append(typed_row)\n",
    "\n",
    "        samples = self._get_strategic_samples(typed_rows)\n",
    "        return self._analyze_combined_samples(samples, use_infer_type=False)\n",
    "\n",
    "    def _analyze_xml(self):\n",
    "        # XML-Analyse bleibt für diese Anforderung unverändert, kann aber auch von der Logik profitieren.\n",
    "        # Implementierung aus der vorherigen Version...\n",
    "        tree = ET.parse(self.file_path)\n",
    "        root = tree.getroot()\n",
    "        def xml_to_dict(elem):\n",
    "            if not list(elem) and elem.text is None and elem.attrib:\n",
    "                return self._analyze_schema(elem.attrib)\n",
    "            if not list(elem):\n",
    "                return self._infer_value_type(elem.text) if elem.text else None\n",
    "            d = {}\n",
    "            for child in list(elem):\n",
    "                child_data = xml_to_dict(child)\n",
    "                if child.tag in d:\n",
    "                    if not isinstance(d[child.tag], list):\n",
    "                        d[child.tag] = [d[child.tag]]\n",
    "                    d[child.tag].append(child_data)\n",
    "                else:\n",
    "                    d[child.tag] = child_data\n",
    "            return d\n",
    "        dict_data = {root.tag: xml_to_dict(root)}\n",
    "        return self._analyze_schema(dict_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9efbec15",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = SchemaAnalyzer(\"../knowledge_base/ChinookData.json\")\n",
    "result = analyzer.analyze()\n",
    "\n",
    "number_of_lines = 10\n",
    "snippets = analyzer.get_file_snippets(number_of_lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "969d070d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Genre': ['array of', {'Name': 'string', 'GenreId': 'integer'}],\n",
      " 'MediaType': ['array of', {'Name': 'string', 'MediaTypeId': 'integer'}],\n",
      " 'Artist': ['array of', {'Name': 'string', 'ArtistId': 'integer'}],\n",
      " 'Album': ['array of',\n",
      "           {'ArtistId': 'integer', 'AlbumId': 'integer', 'Title': 'string'}],\n",
      " 'Track': ['array of',\n",
      "           {'AlbumId': 'integer',\n",
      "            'Bytes': 'integer',\n",
      "            'UnitPrice': 'number',\n",
      "            'TrackId': 'integer',\n",
      "            'MediaTypeId': 'integer',\n",
      "            'Name': 'string',\n",
      "            'Milliseconds': 'integer',\n",
      "            'GenreId': 'integer',\n",
      "            'Composer': 'string'}],\n",
      " 'Employee': ['array of',\n",
      "              {'City': 'string',\n",
      "               'Phone': 'string',\n",
      "               'Email': 'string',\n",
      "               'ReportsTo': ['integer', 'null'],\n",
      "               'HireDate': 'string',\n",
      "               'Title': 'string',\n",
      "               'FirstName': 'string',\n",
      "               'Address': 'string',\n",
      "               'PostalCode': 'string',\n",
      "               'Fax': 'string',\n",
      "               'BirthDate': 'string',\n",
      "               'State': 'string',\n",
      "               'Country': 'string',\n",
      "               'EmployeeId': 'integer',\n",
      "               'LastName': 'string'}],\n",
      " 'Customer': ['array of',\n",
      "              {'City': 'string',\n",
      "               'Phone': 'string',\n",
      "               'Email': 'string',\n",
      "               'LastName': 'string',\n",
      "               'FirstName': 'string',\n",
      "               'Fax': 'string',\n",
      "               'Address': 'string',\n",
      "               'PostalCode': 'string',\n",
      "               'CustomerId': 'integer',\n",
      "               'State': 'string',\n",
      "               'Country': 'string',\n",
      "               'Company': 'string',\n",
      "               'SupportRepId': 'integer'}],\n",
      " 'Invoice': ['array of',\n",
      "             {'BillingAddress': 'string',\n",
      "              'Total': 'number',\n",
      "              'InvoiceDate': 'string',\n",
      "              'BillingState': 'string',\n",
      "              'CustomerId': 'integer',\n",
      "              'BillingCity': 'string',\n",
      "              'BillingPostalCode': 'string',\n",
      "              'BillingCountry': 'string',\n",
      "              'InvoiceId': 'integer'}],\n",
      " 'InvoiceLine': ['array of',\n",
      "                 {'InvoiceLineId': 'integer',\n",
      "                  'TrackId': 'integer',\n",
      "                  'Quantity': 'integer',\n",
      "                  'UnitPrice': 'number',\n",
      "                  'InvoiceId': 'integer'}],\n",
      " 'Playlist': ['array of', {'PlaylistId': 'integer', 'Name': 'string'}],\n",
      " 'PlaylistTrack': ['array of', {'PlaylistId': 'integer', 'TrackId': 'integer'}]}\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "pprint.pprint(result, sort_dicts=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9eb40760",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Holen Sie die Daten-Ausschnitte\n",
    "snippets = analyzer.get_file_snippets(n=10) # n=10 für 10 Zeilen\n",
    "head_str = snippets.get('head')\n",
    "middle_str = snippets.get('middle')\n",
    "tail_str = snippets.get('tail')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5590512d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token-Anzahl: 671\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import tiktoken\n",
    "\n",
    "# 1. Das Schema in einen JSON-String umwandeln\n",
    "schema_str = json.dumps(result, indent=2)\n",
    "\n",
    "# 2. Tokenizer für dein Modell auswählen\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-4\")  # oder \"gpt-3.5-turbo\", etc.\n",
    "\n",
    "# 3. Tokenisieren und zählen\n",
    "tokens = encoding.encode(schema_str)\n",
    "print(\"Token-Anzahl:\", len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "195c0e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2134de35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1eaf246",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    (\n",
    "        \"system\",\n",
    "        \"Your task is to analyze the data and understand its schema, then create a corresponding LinkML schema. This should serve as a transformation proposal for a relational database.\",\n",
    "    ),\n",
    "    (\n",
    "        \"human\",\n",
    "        f\"Can you generate a LinkML schema that reflects the data structure, its format, and relationships? Please provide it in a code block. The schema is: result {schema_str}. The file is a JSON. \\n Tail of the file: {tail_str} \\n Head: {head_str} \\n Middle: {middle_str}\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "ai_msg = llm.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f6ef6c4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Hier ist ein LinkML-Schema, das auf den bereitgestellten Daten basiert:\n",
       "\n",
       "```yaml\n",
       "id: http://example.org/schema/\n",
       "name: music_store\n",
       "\n",
       "types:\n",
       "  id:\n",
       "    base: int\n",
       "    uri: xsd:int\n",
       "  string:\n",
       "    base: str\n",
       "    uri: xsd:string\n",
       "  number:\n",
       "    base: float\n",
       "    uri: xsd:float\n",
       "\n",
       "classes:\n",
       "  Genre:\n",
       "    slots:\n",
       "      - id\n",
       "      - name\n",
       "  MediaType:\n",
       "    slots:\n",
       "      - id\n",
       "      - name\n",
       "  Artist:\n",
       "    slots:\n",
       "      - id\n",
       "      - name\n",
       "  Album:\n",
       "    slots:\n",
       "      - id\n",
       "      - artist_id\n",
       "      - title\n",
       "  Track:\n",
       "    slots:\n",
       "      - id\n",
       "      - album_id\n",
       "      - bytes\n",
       "      - unit_price\n",
       "      - media_type_id\n",
       "      - name\n",
       "      - milliseconds\n",
       "      - genre_id\n",
       "      - composer\n",
       "  Employee:\n",
       "    slots:\n",
       "      - id\n",
       "      - city\n",
       "      - phone\n",
       "      - email\n",
       "      - reports_to\n",
       "      - hire_date\n",
       "      - title\n",
       "      - first_name\n",
       "      - address\n",
       "      - postal_code\n",
       "      - fax\n",
       "      - birth_date\n",
       "      - state\n",
       "      - country\n",
       "      - last_name\n",
       "  Customer:\n",
       "    slots:\n",
       "      - id\n",
       "      - city\n",
       "      - phone\n",
       "      - email\n",
       "      - last_name\n",
       "      - first_name\n",
       "      - fax\n",
       "      - address\n",
       "      - postal_code\n",
       "      - state\n",
       "      - country\n",
       "      - company\n",
       "      - support_rep_id\n",
       "  Invoice:\n",
       "    slots:\n",
       "      - id\n",
       "      - billing_address\n",
       "      - total\n",
       "      - invoice_date\n",
       "      - billing_state\n",
       "      - customer_id\n",
       "      - billing_city\n",
       "      - billing_postal_code\n",
       "      - billing_country\n",
       "  InvoiceLine:\n",
       "    slots:\n",
       "      - id\n",
       "      - track_id\n",
       "      - quantity\n",
       "      - unit_price\n",
       "      - invoice_id\n",
       "  Playlist:\n",
       "    slots:\n",
       "      - id\n",
       "      - name\n",
       "  PlaylistTrack:\n",
       "    slots:\n",
       "      - playlist_id\n",
       "      - track_id\n",
       "\n",
       "slots:\n",
       "  id:\n",
       "    description: The unique identifier\n",
       "    range: id\n",
       "  name:\n",
       "    description: The name\n",
       "    range: string\n",
       "  artist_id:\n",
       "    description: The unique identifier of the artist\n",
       "    range: id\n",
       "  album_id:\n",
       "    description: The unique identifier of the album\n",
       "    range: id\n",
       "  track_id:\n",
       "    description: The unique identifier of the track\n",
       "    range: id\n",
       "  media_type_id:\n",
       "    description: The unique identifier of the media type\n",
       "    range: id\n",
       "  genre_id:\n",
       "    description: The unique identifier of the genre\n",
       "    range: id\n",
       "  bytes:\n",
       "    description: The size of the track in bytes\n",
       "    range: id\n",
       "  unit_price:\n",
       "    description: The price of the track\n",
       "    range: number\n",
       "  milliseconds:\n",
       "    description: The length of the track in milliseconds\n",
       "    range: id\n",
       "  composer:\n",
       "    description: The composer of the track\n",
       "    range: string\n",
       "  # ... repeat for all other slots ...\n",
       "```\n",
       "\n",
       "Bitte beachten Sie, dass Sie die Beschreibungen und Bereiche für jeden Slot an Ihre spezifischen Anforderungen anpassen müssen. Dieses Schema ist nur ein Ausgangspunkt und kann weiter verfeinert und erweitert werden, um Ihre Daten genauer zu modellieren."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "display(Markdown(f\"{ai_msg.content}\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
